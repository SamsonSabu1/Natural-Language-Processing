{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNDCM5mO/yydsJz3gkq1jNY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samsomsabu/NATURAL-LANGUAGE-PROCESSING/blob/main/types_of_similarity_measures_nlp_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_md\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooo78Rwt5ivr",
        "outputId": "aa3a6414-fa14-4e8e-efdd-611681302601"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.5)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0Nm0XEF6AFc",
        "outputId": "6446656e-1e58-4a5c-8f7f-4dc1ca3eb22a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English word embeddings model from spaCy\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "def document_similarity(doc1, doc2):\n",
        "    # Process the documents using spaCy\n",
        "    doc1 = nlp(doc1)\n",
        "    doc2 = nlp(doc2)\n",
        "\n",
        "    # Compute the similarity between the two documents\n",
        "    similarity_score = doc1.similarity(doc2)\n",
        "\n",
        "    return similarity_score\n",
        "\n",
        "# Example usage with the sample documents\n",
        "document1 = \"\"\"\n",
        "Title: The Rise of Artificial Intelligence in Modern Society\n",
        "\n",
        "In recent years, artificial intelligence (AI) has witnessed a remarkable surge in prominence, transforming the landscape of modern society. From virtual assistants to self-driving cars, AI applications have become integral to our daily lives. This document explores the various aspects of AI, including its historical roots, current advancements, and potential future impacts.\n",
        "\n",
        "AI, with its ability to analyze vast amounts of data and perform complex tasks, has found applications in diverse fields such as healthcare, finance, and education. The document delves into specific use cases, highlighting how AI is revolutionizing industries and enhancing efficiency. Additionally, ethical considerations and potential challenges associated with the widespread adoption of AI are discussed, shedding light on the importance of responsible AI development.\n",
        "\n",
        "As we navigate the era of AI, understanding its capabilities and implications is crucial. This document aims to provide insights into the multifaceted world of artificial intelligence, sparking conversations about its role in shaping the future of our society.\n",
        "\"\"\"\n",
        "\n",
        "document2 = \"\"\"\n",
        "Title: Exploring the Ethical Dimensions of Artificial Intelligence\n",
        "\n",
        "The integration of artificial intelligence (AI) into various facets of our lives raises important ethical questions that demand careful consideration. This document focuses on the ethical dimensions of AI, examining the challenges and responsibilities associated with its development and deployment.\n",
        "\n",
        "One key area of concern is the potential bias in AI algorithms, which can perpetuate existing societal inequalities. The document explores real-world examples where AI systems have demonstrated biased behavior and discusses the ethical implications of such instances. Additionally, it delves into the importance of transparency in AI decision-making processes to ensure accountability and mitigate potential harm.\n",
        "\n",
        "The ethical use of AI extends beyond bias, encompassing issues such as privacy, security, and the impact on employment. Through an exploration of these ethical dimensions, this document aims to foster awareness and promote responsible AI practices. By addressing these concerns, we can work towards harnessing the benefits of AI while minimizing its negative consequences on individuals and society.\n",
        "\"\"\"\n",
        "\n",
        "# Calculate similarity score\n",
        "similarity_score = document_similarity(document1, document2)\n",
        "print(f\"Similarity Score: {similarity_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8a1nW3x4b24",
        "outputId": "7c21571d-72e8-4b1d-9e76-a9d155c0bb34"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity Score: 0.9868637487310551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def text_similarity(text1, text2):\n",
        "    # Tokenize and lemmatize the texts\n",
        "    tokens1 = word_tokenize(text1)\n",
        "    tokens2 = word_tokenize(text2)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens1 = [lemmatizer.lemmatize(token) for token in tokens1]\n",
        "    tokens2 = [lemmatizer.lemmatize(token) for token in tokens2]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = stopwords.words('english')\n",
        "    tokens1 = [token for token in tokens1 if token not in stop_words]\n",
        "    tokens2 = [token for token in tokens2 if token not in stop_words]\n",
        "\n",
        "    # Create the TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vector1 = vectorizer.fit_transform(tokens1)\n",
        "    vector2 = vectorizer.transform(tokens2)\n",
        "\n",
        "    # Calculate the cosine similarity\n",
        "    similarity = cosine_similarity(vector1, vector2)\n",
        "\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "bBlqoQC85Py2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_similarity(document2, document1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJAXq9Ov51PT",
        "outputId": "503b2ea8-cd77-48c8-f34c-485f86734ebb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Convert the texts into TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform([document1, document2])\n",
        "\n",
        "# Calculate the cosine similarity between the vectors\n",
        "similarity = cosine_similarity(vectors)\n",
        "print(similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7cpf34358_-",
        "outputId": "b4ae4d8f-b2a4-44c5-86a6-17d393cc5435"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.66085092]\n",
            " [0.66085092 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the documents and create tokens\n",
        "doc1_tokens=set(document1.lower().split())\n",
        "doc2_tokens=set(document2.lower().split())\n",
        "\n",
        "#Print the tokens\n",
        "print(doc1_tokens,doc2_tokens)\n",
        "\n",
        "\n",
        "# Calculate the Jaccard Similarity\n",
        "jaccard_similarity=  len(doc1_tokens.intersection(doc2_tokens))/len(doc1_tokens.union(doc2_tokens))\n",
        "\n",
        "# Print the Jaccard Simialrity score\n",
        "print(jaccard_similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fVFOrvU6pQZ",
        "outputId": "97e16d3a-f6a5-4ae1-c3d5-41f369f6b93a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'applications', 'from', 'on', 'understanding', 'society', 'remarkable', 'recent', 'the', 'prominence,', 'about', 'diverse', 'our', 'specific', 'aspects', 'aims', 'associated', 'daily', 'modern', 'analyze', 'are', 'insights', 'title:', 'future', 'document', 'shedding', 'world', 'roots,', 'widespread', 'to', '(ai)', 'responsible', 'crucial.', 'artificial', 'enhancing', 'this', 'ethical', 'including', 'importance', 'highlighting', 'complex', 'historical', 'capabilities', 'of', 'ai', 'provide', 'amounts', 'landscape', 'with', 'education.', 'such', 'considerations', 'found', 'multifaceted', 'cars,', 'use', 'various', 'rise', 'data', 'we', 'its', 'and', 'virtual', 'fields', 'intelligence', 'perform', 'tasks,', 'witnessed', 'finance,', 'delves', 'era', 'ability', 'as', 'healthcare,', 'intelligence,', 'in', 'role', 'a', 'years,', 'ai,', 'implications', 'assistants', 'navigate', 'lives.', 'additionally,', 'current', 'advancements,', 'potential', 'cases,', 'vast', 'sparking', 'is', 'impacts.', 'revolutionizing', 'discussed,', 'have', 'industries', 'society.', 'light', 'transforming', 'how', 'development.', 'surge', 'challenges', 'has', 'efficiency.', 'conversations', 'shaping', 'integral', 'explores', 'self-driving', 'into', 'adoption', 'become'} {'behavior', 'responsibilities', 'an', 'on', 'instances.', 'negative', 'impact', 'while', 'consideration.', 'demonstrated', 'where', 'work', 'societal', 'consequences', 'the', 'encompassing', 'awareness', 'processes', 'promote', 'dimensions', 'foster', 'security,', 'dimensions,', 'questions', 'harm.', 'our', 'privacy,', 'issues', 'aims', 'associated', 'algorithms,', 'title:', 'discusses', 'facets', 'document', 'practices.', 'examining', 'development', 'key', 'existing', 'employment.', 'to', 'that', '(ai)', 'systems', 'responsible', 'artificial', 'this', 'ethical', 'inequalities.', 'decision-making', 'importance', 'bias', 'of', 'ai', 'through', 'real-world', 'harnessing', 'integration', 'with', 'important', 'addressing', 'such', 'perpetuate', 'these', 'use', 'various', 'we', 'its', 'exploring', 'and', 'towards', 'benefits', 'intelligence', 'delves', 'examples', 'biased', 'as', 'concerns,', 'raises', 'accountability', 'deployment.', 'in', 'mitigate', 'which', 'careful', 'focuses', 'ai,', 'demand', 'implications', 'transparency', 'by', 'additionally,', 'individuals', 'potential', 'one', 'ensure', 'concern', 'area', 'is', 'beyond', 'have', 'extends', 'bias,', 'exploration', 'society.', 'lives', 'minimizing', 'challenges', 'explores', 'into', 'it', 'can'}\n",
            "0.19576719576719576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oksOCtwGFHXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is text similarity?\n",
        "\n",
        "\n",
        "Text similarity measures how much the meaning or content of two pieces of text are the same. It measures the degree to which two texts are semantically related. There are many ways to measure text similarity, including techniques such as cosine similarity, Levenshtein distance, and the Jaccard index. These techniques can be used to compare texts in different languages or within the same language.\n",
        "\n",
        "\n",
        "Text similarity can be used in many ways to find information, process natural language, and translate between languages automatically. For example, a search engine might use text similarity to rank search results based on their relevance to the query. In natural language processing, text similarity can be used to identify synonyms or generate text similar in style or meaning to a given text. Text similarity can be used in machine translation to find similar translations to the source text.\n",
        "\n",
        "\n",
        "What is text similarity used for?\n",
        "\n",
        "\n",
        "Some common use cases include:\n",
        "\n",
        "Plagiarism detection: Text similarity can be used to identify instances of plagiarism by comparing the similarity of a piece of text to other known texts.\n",
        "Document classification: Text similarity can be used to classify documents based on their content. For example, a document classification system might use text similarity to determine whether a document is relevant to a particular topic.\n",
        "\n",
        "\n",
        "Information retrieval: Text similarity can be used to identify relevant documents in a search engine or database. For example, a search query for “car” might return documents that contain similar words or phrases, such as “automobile” or “vehicle.”\n",
        "\n",
        "\n",
        "Language translation: Text similarity can be used to improve the accuracy of machine translation systems by comparing the similarity of a translated text to the original text.\n",
        "\n",
        "\n",
        "Sentiment analysis: Text similarity can be used to identify the sentiment of a piece of text by comparing it to a set of pre-classified texts with known sentiments.\n",
        "\n",
        "\n",
        "Summarization: Text similarity can be used to summarise a document by identifying the most important sentences and phrases within the document.\n",
        "Different text similarity algorithms\n",
        "\n",
        "\n",
        "Many different algorithms can be used to measure text similarity. Some common ones include:\n",
        "\n",
        "1. Cosine similarity\n",
        "\n",
        "\n",
        "This measures the similarity between two texts based on the angle between their word vectors. It is often used with term frequency-inverse document frequency (TF-IDF) vectors, representing each word’s importance in a document.\n",
        "\n",
        "Cosine similarity measures the similarity between two non-zero vectors of an inner product space. In the context of document similarity, it is often used to measure the similarity between two documents represented as vectors of word frequencies. The cosine similarity between two vectors is calculated as the cosine of the angle between them.\n",
        "\n",
        "To compute the cosine similarity between two documents, first, a vector representation of each document is constructed, where each dimension of the vector corresponds to a word in the document, and the value of the dimension represents the frequency of that word in the document. The vectors are then normalized to have a unit length. The cosine similarity between the two documents is then calculated as the dot product of the two vectors divided by the product of their lengths.\n",
        "\n",
        "The resulting cosine similarity value ranges from -1 to 1, where -1 indicates completely dissimilar documents, and 1 indicates identical documents. A value of 0 indicates that the two documents are orthogonal and have no similarity.\n",
        "\n",
        "Cosine similarity is widely used in natural language processing and information retrieval, particularly in document clustering, classification, and recommendation systems.\n",
        "\n",
        "2. Levenshtein distance\n",
        "\n",
        "\n",
        "Levenshtein distance, or edit distance, measures the difference between two strings. It is the minimum number of single-character insertions, deletions, or substitutions required to transform one string into another.\n",
        "\n",
        "For example, the Levenshtein distance between “kitten” and “sitting” is 3, since three single-character edits are required to transform “kitten” into “sitting”: substitute “s” for “k”, substitute “i” for “e”, and insert “g” at the end.\n",
        "\n",
        "Levenshtein distance is used in various applications such as spell-checking, string matching, and DNA analysis.\n",
        "\n",
        "3. Jaccard index\n",
        "The Jaccard index, or the Jaccard similarity coefficient, measures the similarity between two sets. It is defined as the ratio of the size of the intersection of the sets to the size of the union of the sets. In other words, it is the proportion of common elements between two sets.\n",
        "\n",
        "The Jaccard index is particularly useful when the presence or absence of elements in the sets is more important than their frequency or order. For example, it can be used to compare the similarity of two documents by considering the sets of words that appear in each document.\n",
        "\n",
        "The Jaccard index is calculated as follows:\n",
        "\n",
        "J(A,B) = |A ∩ B| / |A ∪ B|\n",
        "\n",
        "where A and B are sets, and |A| and |B| represent the cardinality or size of the sets.\n",
        "\n",
        "The resulting value of the Jaccard index ranges from 0 to 1, where 0 indicates no common elements between the sets, and 1 indicates that the sets are identical.\n",
        "\n",
        "The Jaccard index is widely used in various applications such as information retrieval, data mining, and pattern recognition. It is particularly useful when dealing with sparse or high-dimensional data, where the presence or absence of features is more important than their actual values.\n",
        "\n",
        "4. Euclidean distance\n",
        "Euclidean distance is a measure of the distance between two points in a Euclidean space. It is calculated as the square root of the sum of the squares of the differences between the corresponding coordinates of the two points.\n",
        "\n",
        "For example, the Euclidean distance between two points (x1, y1) and (x2, y2) in a two-dimensional space is given by:\n",
        "\n",
        "euclidean distance = sqrt((x2 - x1)^2 + (y2 - y1)^2)\n",
        "The Euclidean distance can be extended to spaces of any dimension. It is commonly used in machine learning and data analysis to measure the similarity between two vectors in a high-dimensional space.\n",
        "\n",
        "In the context of document similarity, the Euclidean distance can be used to compare the frequency of words in two documents represented as vectors of word frequencies. In this case, the Euclidean distance between the two vectors is calculated as the square root of the sum of the squared differences between the corresponding frequency values in the two vectors.\n",
        "\n",
        "The resulting value of Euclidean distance ranges from 0 to infinity, where 0 indicates identical vectors and larger values indicate greater dissimilarity between the vectors.\n",
        "\n",
        "Euclidean distance is widely used in various applications such as clustering, classification, and anomaly detection. It is particularly useful when dealing with continuous variables or data that can be represented as vectors in a high-dimensional space.\n",
        "\n",
        "5. Hamming distance\n",
        "Hamming distance measures the difference between two strings of equal length. It is defined as the number of positions at which the corresponding symbols differ. In other words, it is the minimum number of single-character substitutions required to transform one string into another of equal length.\n",
        "\n",
        "For example, the Hamming distance between “101010” and “111011” is 2, since two positions differ between the two strings: the second and fifth.\n",
        "\n",
        "Hamming distance is used in various applications such as error-correcting codes, coding theory, and cryptography. It can also be used to compare the similarity of binary strings, such as DNA sequences.\n",
        "\n",
        "In computer science, Hamming distance is often used as a metric to measure the quality of codes. For example, in error-correcting codes, the minimum Hamming distance between codewords determines the number of errors that can be corrected by the code. Codes with a larger minimum Hamming distance are more robust to errors.\n",
        "\n",
        "The Hamming distance can be calculated using a simple algorithm that compares the symbols at each position in the two strings and counts the number of positions where they differ.\n",
        "\n",
        "6. Word embeddings\n",
        "Word embeddings are distributed representations of words in a natural language. They represent words as vectors of real numbers, where each vector dimension represents a different feature or aspect of the word’s meaning. Word embeddings are often fundamental in many natural language processing tasks, such as machine translation, text classification, and information retrieval.\n",
        "\n",
        "Word embeddings are typically learned from large corpora of text data using neural network models, such as the famous Word2Vec model or GloVe. These models map words to a high-dimensional space where semantically similar words are mapped to nearby points. The learned embeddings capture both the syntactic and semantic relationships between words and can capture complex analogies and relationships between words.\n",
        "\n",
        "Word embeddings have several advantages over traditional methods for representing words in natural language processing, such as one-hot encoding or continuous bag-of-words representations. For example:\n",
        "\n",
        "They are dense, meaning they are more space-efficient than sparse representations like one-hot encoding.\n",
        "They can capture semantic relationships between words that cannot be easily captured by traditional methods.\n",
        "They can infer relationships between words or generate new representations of words not seen in the training data.\n",
        "Many pre-trained word embeddings are available, which can be used for various NLP tasks. Additionally, custom word embeddings can be trained on specific domains or datasets to improve performance on specific tasks.\n",
        "\n",
        "7. Pre-trained language models\n",
        "Pre-trained language models are powerful tools for text similarity tasks, as they can learn high-quality representations of text that capture both semantic and syntactic information. Here are some of the most widely used pre-trained language models for text similarity tasks:\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers): BERT is a transformer-based pre-trained language model widely used for various natural language processing tasks, including text similarity. It has been shown to outperform previous state-of-the-art methods on several benchmark datasets.\n",
        "RoBERTa (Robustly Optimized BERT Pretraining Approach): RoBERTa is a variant of BERT that is pre-trained using additional data and training strategies. It has achieved state-of-the-art performance on several text similarity benchmarks.\n",
        "DistilBERT: DistilBERT is a smaller and faster version of BERT trained using a knowledge distillation technique. It has achieved competitive performance on several text similarity benchmarks much faster than BERT.\n",
        "USE (Universal Sentence Encoder): USE is a pre-trained model developed by Google that can encode sentences into fixed-length vectors. It can be used for text similarity tasks by computing the cosine similarity between the sentence embeddings.\n",
        "ALBERT (A Lite BERT): ALBERT is a variant of BERT that reduces the number of parameters and improves training efficiency while maintaining comparable performance.\n",
        "These pre-trained language models can be fine-tuned on specific text similarity tasks using transfer learning, which involves training the model on a smaller dataset of labelled examples. Fine-tuning can further improve the performance of these models on specific tasks."
      ],
      "metadata": {
        "id": "NAAdPRFFKERf"
      }
    }
  ]
}